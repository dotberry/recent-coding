# Generated by Colaboratory
# There is a discrepancy between the number of defective and non-defective items.

import os
import numpy as np
import tensorflow as tf
from google.colab import drive
from sklearn.metrics import classification_report
from sklearn.model_selection import train_test_split, KFold
from tensorflow.keras.layers import Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ReduceLROnPlateau, EarlyStopping
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.models import Model
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense
from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.optimizers import Adam


drive.mount('/content/drive')
!cp "/content/drive/MyDrive/Colab Notebooks/screws.zip" "."
!unzip "screws.zip"

normal_dir = "/content/archive/train/good"
defective_dir = "/content/archive/train/not-good"

# Make rotated_good and rotated_not_good folders
if not os.path.exists('/content/archive/train/rotated_good'):
    os.makedirs('/content/archive/train/rotated_good')

if not os.path.exists('/content/archive/train/rotated_not_good'):
    os.makedirs('/content/archive/train/rotated_not_good')

# Move image files to a subfolder
def restructure_directory(dir_path, sub_dir_name):
    new_sub_dir_path = os.path.join(dir_path, sub_dir_name)

    if not os.path.exists(new_sub_dir_path):
        os.makedirs(new_sub_dir_path)

    for filename in os.listdir(dir_path):
        file_path = os.path.join(dir_path, filename)
        if os.path.isfile(file_path) and not filename.startswith('.'):
            os.rename(file_path, os.path.join(new_sub_dir_path, filename))

restructure_directory(normal_dir, "good_class")
restructure_directory(defective_dir, "not-good_class")

def load_images_from_directory(normal_dir, defective_dir, target_size=(128, 128)):
    # Obtain lists for normal and defective images
    normal_images = [os.path.join(normal_dir, "good_class", img) for img in os.listdir(os.path.join(normal_dir, "good_class"))]
    defective_images = [os.path.join(defective_dir, "not-good_class", img) for img in os.listdir(os.path.join(defective_dir, "not-good_class"))]

    # Change images to numpy array
    x_normal = [np.squeeze(tf.image.resize(np.array(tf.image.decode_image(tf.io.read_file(img))), target_size)) for img in normal_images]
    x_defective = [np.squeeze(tf.image.resize(np.array(tf.image.decode_image(tf.io.read_file(img))), target_size)) for img in defective_images]

    # Convert images to 3 channels if they are not
    x_normal = [image if len(image.shape) == 3 and image.shape[2] == 3 else np.repeat(image[:,:,np.newaxis], 3, axis=2) for image in x_normal]
    x_defective = [image if len(image.shape) == 3 and image.shape[2] == 3 else np.repeat(image[:,:,np.newaxis], 3, axis=2) for image in x_defective]

    # Create labels: good samples: 0, not good samples: 1
    y_normal = np.zeros(len(x_normal))
    y_defective = np.ones(len(x_defective))

    x = np.concatenate([x_normal, x_defective], axis=0)
    y = np.concatenate([y_normal, y_defective], axis=0)

    return x, y

# ImageDataGenerator for not good-samples
not_good_datagen = ImageDataGenerator(rotation_range=360, fill_mode='nearest')

not_good_generator = not_good_datagen.flow_from_directory(
    directory=defective_dir,
    target_size=(128, 128),
    class_mode='binary',
    batch_size=1,
    save_to_dir='/content/archive/train/rotated_not_good',
    save_prefix='aug',
    save_format='jpeg')

# Generate and save augmented images
for i in range(120):
    x_not_good, y_not_good = next(not_good_generator)

# ImageDataGenerator for good samples
good_datagen = ImageDataGenerator(rotation_range=180, fill_mode='nearest')

good_generator = good_datagen.flow_from_directory(
    directory=normal_dir,
    target_size=(128, 128),
    class_mode='binary',
    batch_size=1,
    save_to_dir='/content/archive/train/rotated_good',
    save_prefix='aug',
    save_format='jpeg')

# Generate and save augmented images
for i in range(36):
    x_good, y_good = next(good_generator)

x, y = load_images_from_directory(normal_dir, defective_dir, target_size=(128, 128))

# Fine tuning with ResNet50
def resnet50_model(input_shape, learning_rate=0.001):
    base_model = ResNet50(weights='imagenet', include_top=False, input_shape=input_shape)
    for layer in base_model.layers:
        layer.trainable = False

    x = base_model.output
    x = GlobalAveragePooling2D()(x)
    x = Dense(256, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    x = Dense(128, activation='relu')(x)
    x = BatchNormalization()(x)
    x = Dropout(0.5)(x)
    predictions = Dense(1, activation='sigmoid')(x)

    model = Model(inputs=base_model.input, outputs=predictions)
    optimizer = Adam(learning_rate=learning_rate)
    model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Cross-validation to find the best learning rate
learning_rates = [0.1, 0.01, 0.001, 0.0001]
n_splits = 3

best_lr = 0
best_accuracy = 0

x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

for lr in learning_rates:
    kf = KFold(n_splits=n_splits, shuffle=True, random_state=42)
    accuracies = []

    for train_index, val_index in kf.split(x):
        x_train_fold, x_val_fold = x[train_index], x[val_index]
        y_train_fold, y_val_fold = y[train_index], y[val_index]

        model = resnet50_model(input_shape=x_train_fold.shape[1:], learning_rate=lr)
        model.fit(x_train_fold, y_train_fold, epochs=10, batch_size=128, verbose=0)  

        _, accuracy = model.evaluate(x_val_fold, y_val_fold, verbose=0)
        accuracies.append(accuracy)

    avg_accuracy = np.mean(accuracies)
    if avg_accuracy > best_accuracy:
        best_accuracy = avg_accuracy
        best_lr = lr

print(f"Best Learning Rate: {best_lr} with accuracy of {best_accuracy:.2f}")

# Train the final model with best learning rate
input_shape = x_train.shape[1:]
final_model = resnet50_model(input_shape, learning_rate=best_lr)

# Callbacks
learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss',
                                            patience=3,
                                            verbose=1,
                                            factor=0.5,
                                            min_lr=0.00001)

early_stopping = EarlyStopping(monitor='val_loss', patience=7)
callbacks = [learning_rate_reduction, early_stopping]

final_model.fit(x_train, y_train, epochs=20, batch_size=128, validation_data=(x_test, y_test), callbacks=callbacks)

# Evaluate the final model
loss, accuracy = final_model.evaluate(x_test, y_test)
print(f"Test Accuracy: {accuracy * 100:.2f}%")

# save the trained model
final_model.save('trained_model.h5')

def evaluate_with_classification_report(model, x_test, y_test):
    # Get model predictions
    y_pred = model.predict(x_test).ravel()
    y_pred_classes = (y_pred > 0.5).astype(int)

    # Generate classification report
    report = classification_report(y_test, y_pred_classes, target_names=['Good', 'Not Good'])

    return report

report = evaluate_with_classification_report(final_model, x_test, y_test)
print(report)
